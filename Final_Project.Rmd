---
title: "Final_Project"
author: "Chaganti Venkata Kanishk"
date: "4/28/2022"
output: pdf_document
---

# Libraries 
```{r}
#install.packages("performance", dependencies = T)
library(e1071)
library(faraway)
library(MASS)
library(car)
library(patchwork)
library(tidyverse)
library(performance)
library(randomForest)
library(corrplot)
library(caret)
library(see)
library(ggplot2)
```

#  Visualization Of the dataset

```{r}
red <- read.csv("winequality-red.csv")
head(red)
```

```{r}
names(red)
```

```{r}
table(red$quality)
```


# Scaterplots 
```{r}
red$quality <- as.integer(red$quality)

for (i in c(1:11)){
  plot(red[, i], red[, "quality"], xlab = names(red)[i],
         ylab = "quality")
  abline(lm(red[, "quality"] ~ red[ ,i]), lty = 2, lwd = 2)
}

```


```{r}
lm <- lm(quality~.,data = red)
summary(lm)
```


# assumptions

```{r}
ggplot(lm, aes(fitted(lm), resid(lm) )) + geom_point() +
geom_hline(yintercept=0)
qqnorm(rstandard(lm))
qqline(rstandard(lm))
```





```{r}
set.seed(99)
check_model(lm)
```



# Box-Cox Transformations

```{r}
boxcox(lm, lambda=seq(0.3, 0.65, by=0.05))
summary(powerTransform(lm))
```

# multicollinearity 

```{r}
vif_values<- round(vif(lm),2)
vif_values
```

```{r}
barplot(vif_values, main = "VIF Values", horiz = TRUE, col = "steelblue")
abline(v = 5, lwd = 3, lty = 2)
```


# Corelation matrix

```{r}
reduced_red <- red[, c("fixed.acidity", "volatile.acidity","citric.acid","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","pH","sulphates")]
cor(reduced_red)
```
# variable selection

```{r}
lm2<- step(lm)
summary(lm2)
AIC(lm,lm2)
```

# Hypothesis Testing 

$H_0$ (null hypothesis): none of the predictor variables have a statistically significant relationship with quality of the red wine.
$H_0:\beta_1=\beta2=...\beta_{11}=0$

$H_A$(alternative hypotheses): at-least one of the predictor variables have a statistically significant relationship with the quality of the red wine.
$H_A:$ at least one $\beta_j\ne0$ 

```{r}
lm1<- lm(quality~1, data= red)
anova(lm1,lm)
```

Comment:

From the anova table, we can observe that the p-value <0.001, so we the null hupothesis and say that $\beta_1=\beta2=...\beta_{11}=0$. Thus we conclude that atleast one predictor is associated for determining Quality.

## partial F-test 

Null Hypothesis: variables: fixed.acidity,citric.acid,residual.sugar,density does not have statistically significant relationship with quality of the red wine.
$H_0:\beta_1=\beta_3=\beta_4=\beta_8=0$


Alternate Hypothesis: variables: fixed.acidity,citric.acid,residual.sugar,density have statistically significant relationship with the quality of the red wine.
$H_A:$$\beta_1\ne0$ or $\beta_3\ne0$ or $\beta_4\ne0$ or $\beta_8\ne0$

```{r}
anova(lm2,lm)
```

Since the p-value=0.6124 is large, so do not reject the null hypothesis that $H_0:\beta_1=\beta_3=\beta_4=\beta_8=0$. So we can remove the subset of predictors (fixed.acidity,citric.acid,residual.sugar,density)from the model.


# Model Selection 



# Spliting the data 

```{r}
split<- sample(c(rep(0, 0.7 * nrow(red)), rep(1, 0.3 * nrow(red))))
Train <- red[split == 0, ]
test <- red[split== 1, ] 
```

## Support Vector Machine

```{r}
svm.grid <- expand.grid(C = 2^(1:3), sigma = seq(0.1, 1, length = 10))
svm.train <- train(factor(quality) ~ ., data = Train, method = "svmRadial",
                    tuneGrid = svm.grid,
                   preProcess = c("center", "scale"))
summary(svm.train)

```

```{r}
plot(svm.train)
```

```{r}
svm.predict <- predict(svm.train, newdata = test)
confusionMatrix(svm.predict,factor(test$quality))
```


## Random forest 

```{r}
rf<- randomForest(factor(quality) ~ .,data = Train)
summary(rf)
plot(rf)
```

```{r}
rf.predict <- predict(rf,newdata=test)
confusionMatrix(rf.predict,factor(test$quality))
```




# Conclution 

